
@article{
    longhini2024clothsplatting,
    title={DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting},
    author={Holly Dinkel and Marcel B{\"u}sching and Alberta Longhini and Brian Coltin and Trey Smith and Danica Kragic and M{\r{a}}rten Bj{\"o}rkman and Timothy Bretl},
    journal={ICRA 2024 Workshop on 3D Visual Representations for Robot Manipulation},
    abstract={This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.},
    month=may,
    year={2025},
    html = {https://arxiv.org/abs/2505.08644},
    url = {https://arxiv.org/abs/2505.08644},
    selected={true},
    preview={dlo-splatting.png}
}




@article{
    longhini2024clothsplatting,
    title={Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision},
    author={Alberta Longhini and Marcel B{\"u}sching and Bardienus Pieter Duisterhof and Jens Lundell and Jeffrey Ichnowski and M{\r{a}}rten Bj{\"o}rkman and Danica Kragic},
    journal={8th Annual Conference on Robot Learning (CoRL)},
    abstract={We introduce Cloth-Splatting, a method for estimating 3D states of cloth from RGB images through a prediction-update framework. Cloth-Splatting leverages an action-conditioned dynamics model for predicting future states and uses 3D Gaussian Splatting to update the predicted states. Our key insight is that coupling a 3D mesh-based representation with Gaussian Splatting allows us to define a differentiable map between the cloth's state space and the image space. This enables the use of gradient-based optimization techniques to refine inaccurate state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting not only improves state estimation accuracy over current baselines but also reduces convergence time by 85%.},
    month=nov,
    year={2024},
    html = {https://kth-rpl.github.io/cloth-splatting/},
    url = {https://kth-rpl.github.io/cloth-splatting/},
    selected={true},
    preview={cloth-splatting.gif}
}


@article{flowibr,
    title = {{FlowIBR}: {Leveraging} {Pre}-{Training} for {Efficient} {Neural} {Image}-{Based} {Rendering} of {Dynamic} {Scenes}},
    shorttitle = {{FlowIBR}},
    author = {Büsching, Marcel and Bengtson, Josef and Nilsson, David and Björkman, Mårten},
    journal={CVPR 2024 Workshop on Efficient Deep Learning for Computer Vision},
    abstract = {We introduce a novel approach for monocular novel view synthesis of dynamic scenes. Existing techniques already show impressive rendering quality but tend to focus on optimization within a single scene without leveraging prior knowledge. This limitation has been primarily attributed to the lack of datasets of dynamic scenes available for training and the diversity of scene dynamics. Our method FlowIBR circumvents these issues by integrating a neural image-based rendering method, pre-trained on a large corpus of widely available static scenes, with a per-scene optimized scene flow field. Utilizing this flow field, we bend the camera rays to counteract the scene dynamics, thereby presenting the dynamic scene as if it were static to the rendering network. The proposed method reduces per-scene optimization time by an order of magnitude, achieving comparable results to existing methods - all on a single consumer-grade GPU.},
    month = jun,
    year = {2024},
    html = {https://flowibr.github.io},
    url = {https://flowibr.github.io},
    selected={true},
    preview={flowibr.gif}
}


@article{longhini2024distilling,
    title={Distilling Semantic Features for 3D Cloth Representations from Vision Foundation Models},
    author={Longhini, Alberta and Büsching, Marcel and Duisterhof, Bardienus Pieter and Jeffrey Ichnowski and Björkman, Mårten and Kragic, Danica},
    journal={ICRA 2024 Workshop on 3D Visual Representations for Robot Manipulation},
    abstract = {This study explores the use of vision foundation models to enhance 3D representations of cloth-like deformable objects. By focusing on the distillation of semantic information from RGB images, we examine the potential of pre-trained Visual-Language Models in capturing complex folded configurations of cloth. Our investigation reveals the challenges and preliminary successes in leveraging semantic information to improve the understanding and tracking of deformable object states.},
    year={2024},
    html={https://openreview.net/forum?id=1mwJlHsS19},
    url={https://openreview.net/forum?id=1mwJlHsS19},
    selected={true},
    preview={distilling_semantic.png}
}
